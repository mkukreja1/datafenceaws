{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "raw",
			"source": "\n# Glue Studio Notebook\nYou are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n\n## Available Magics\n|          Magic              |   Type       |                                                                        Description                                                                        |\n|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n| %region                     |  String      |  Specify the AWS region in which to initialize a session.                                                                                                 |\n| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0).                               |\n| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n| %etl                        |  String      |  Changes the session type to Glue ETL.                                                                                                                    |\n| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n| %stop_session               |              |  Stops the current session.                                                                                                                               |\n| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X.                                                                           |\n| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer.                      |",
			"metadata": {
				"editable": false,
				"deletable": false
			}
		},
		{
			"cell_type": "code",
			"source": "# Setup Spark and Glue configurations\n\n%glue_version 3.0\n%spark_conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n%spark_conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\n%number_of_workers 2\n\n%%configure\n{\n  \"--datalake-formats\": \"delta\"\n}\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Setup Python and Spark libraries\nimport sys\nfrom awsglue.transforms import *\nfrom pyspark.sql.functions import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nfrom delta.tables import *\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, array, ArrayType, DateType, TimestampType, FloatType\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Send arguments to the job\n\nsys.argv+=[\"--S3_BUCKET\", \"aws-analytics-course\"]\nsys.argv+=[\"--BRONZE_LAYER_NAMESPACE\", \"bronze/dms\"]\nsys.argv+=[\"--SCRATCH_LAYER_NAMESPACE\", \"temp/delta\"]\nsys.argv+=[\"--STORE_SALES_FOLDER\", \"sales\"]\nsys.argv+=[\"--INCREMENTAL_DATA_FOLDER\", \"2023/01/12/19\"]",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Read job arguments\n\nargs = getResolvedOptions(sys.argv,[\"S3_BUCKET\", \"BRONZE_LAYER_NAMESPACE\", \"SCRATCH_LAYER_NAMESPACE\", \"STORE_SALES_FOLDER\", \"INCREMENTAL_DATA_FOLDER\"])\nDELTA_TABLE_PATH=\"s3://\" + args['S3_BUCKET'] + \"/\" + args['SCRATCH_LAYER_NAMESPACE'] + \"/\"\nprint(DELTA_TABLE_PATH)",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Option 1 - Read file into a Dataframe by declaring a schema for the sales_order files\n\nSALES_ORDERS_PATH=\"s3://\" + args['S3_BUCKET'] + \"/\" + args['BRONZE_LAYER_NAMESPACE'] + \"/\" + args['STORE_SALES_FOLDER'] + \"/\" + \"store_orders\"\nSALES_ORDERS_SCHEMA =[\n    ('Op', StringType()),\n    ('order_number', IntegerType()),\n    ('customer_id', IntegerType()),\n    ('product_id', IntegerType()),\n    ('order_date', StringType()),    \n    ('units', IntegerType()),\n    ('sale_price', FloatType()),\n    ('currency', StringType()),\n    ('order_mode', StringType())\n]\n\nfields = [StructField(*field) for field in SALES_ORDERS_SCHEMA]\nschema = StructType(fields)\ndf_read_data_incremental = spark.read                          \\\n                                .option(\"header\", \"true\")      \\\n                                .csv(SALES_ORDERS_PATH + \"/\" + args['INCREMENTAL_DATA_FOLDER'] + \"/\" + \"*.csv\",schema=schema)\n\ndf_read_data_incremental.show(3)\ndf_read_data_incremental.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Option 2 - Read file into a Dataframe by having Spark infer the schema for the sales_order files\n\nSALES_ORDERS_PATH=\"s3://\" + args['S3_BUCKET'] + \"/\" + args['BRONZE_LAYER_NAMESPACE'] + \"/\" + args['STORE_SALES_FOLDER'] + \"/\" + \"store_orders\"\n\ndf_read_data_incremental = spark.read                             \\\n                                .option(\"header\", \"true\")         \\\n                                .option(\"inferSchema\", \"true\")    \\\n                                .csv(SALES_ORDERS_PATH + \"/\" + args['INCREMENTAL_DATA_FOLDER'] + \"/\" + \"*.csv\")\n\ndf_read_data_incremental.show(3)\ndf_read_data_incremental.printSchema()\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Change the column types - convert order_date from string to date, updated_at from string to timestamp\n\ndf_read_data_incremental = df_read_data_incremental.withColumn(\"order_date\", to_date(df_read_data_incremental.order_date,  'MM/dd/yyyy'))\ndf_read_data_incremental = df_read_data_incremental.withColumn(\"updated_at\", to_timestamp(df_read_data_incremental.updated_at,  'yyyy-MM-dd HH:mm:ss'))\ndf_read_data_incremental.show(3)\ndf_read_data_incremental.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Use Delta framework to either create a delta table or merge data into an existing table\n\ntry:\n    deltaTable = DeltaTable.forPath(spark, DELTA_TABLE_PATH + \"/\" + \"store_orders\")\n    if deltaTable:\n        print(\"Delta table exists\")\n        df_read_data_incremental = spark.read                             \\\n                                        .option(\"header\", \"true\")         \\\n                                        .option(\"inferSchema\", \"true\")    \\\n                                        .csv(SALES_ORDERS_PATH + \"/\" + args['INCREMENTAL_DATA_FOLDER'] + \"/\" + \"*.csv\")\n        df_read_data_incremental = df_read_data_incremental.withColumn(\"order_date\", to_date(df_read_data_incremental.order_date,  'MM/dd/yyyy'))\n        df_read_data_incremental = df_read_data_incremental.withColumn(\"updated_at\", to_timestamp(df_read_data_incremental.updated_at,  'yyyy-MM-dd HH:mm:ss'))\n        df_read_data_incremental.show(10)\n        deltaTable.alias(\"store_orders\").merge(\n        df_read_data_incremental.alias(\"store_orders_incremental\"),\n                \"store_orders.order_number = store_orders_incremental.order_number\")                     \\\n                .whenMatchedUpdate(set = {\"Op\":         \"store_orders_incremental.Op\",                   \\\n                                          \"order_number\":     \"store_orders_incremental.order_number\",   \\\n                                          \"customer_id\":      \"store_orders_incremental.customer_id\",    \\\n                                          \"product_id\":       \"store_orders_incremental.product_id\",     \\\n                                          \"order_date\":       \"store_orders_incremental.order_date\",     \\\n                                          \"units\":            \"store_orders_incremental.units\",          \\\n                                          \"sale_price\":       \"store_orders_incremental.sale_price\",     \\\n                                          \"currency\":         \"store_orders_incremental.currency\",       \\\n                                          \"order_mode\":       \"store_orders_incremental.order_mode\",     \\\n                                          \"updated_at\":       \"store_orders_incremental.updated_at\"} )   \\\n                .whenNotMatchedInsert(values =                                                           \\\n                   {                                                    \n                                          \"Op\":         \"store_orders_incremental.Op\",                   \\\n                                          \"order_number\":   \"store_orders_incremental.order_number\",     \\\n                                          \"customer_id\":      \"store_orders_incremental.customer_id\",    \\\n                                          \"product_id\":       \"store_orders_incremental.product_id\",     \\\n                                          \"order_date\":       \"store_orders_incremental.order_date\",     \\\n                                          \"units\":            \"store_orders_incremental.units\",          \\\n                                          \"sale_price\":       \"store_orders_incremental.sale_price\",     \\\n                                          \"currency\":         \"store_orders_incremental.currency\",       \\\n                                          \"order_mode\":       \"store_orders_incremental.order_mode\",     \\\n                                          \"updated_at\":       \"store_orders_incremental.updated_at\"      \\\n                   }                                                                                     \\\n                 ).execute()\nexcept:\n    print(\"Delta table does not exist\")\n    df_read_data_full = spark.read                          \\\n                             .option(\"header\", \"true\")      \\\n                             .option(\"inferSchema\", \"true\") \\\n                             .csv(SALES_ORDERS_PATH + \"/\" + \"LOAD00000001.csv\",schema=schema)\n    \n    df_read_data_full = df_read_data_full.withColumn(\"order_date\", to_date(df_read_data_full.order_date,  'MM/dd/yyyy'))\n    df_read_data_full = df_read_data_full.withColumn(\"updated_at\", lit(current_timestamp()))\n    df_read_data_full.write.format(\"delta\").save(DELTA_TABLE_PATH + \"/\" + \"store_orders\")\n    df_read_data_full.show(5)",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "dfread=spark.read.format(\"delta\").load(DELTA_TABLE_PATH + \"/\" + \"store_orders\")\ndfread.createOrReplaceTempView( \"store_orders\")\nprint(dfread.count())",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "%%sql\nSELECT * FROM store_orders LIMIT 5;",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "%%sql\nSELECT * FROM store_orders WHERE order_number = 2345;",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Show history of the table\n\ndeltaTable = DeltaTable.forPath(spark, DELTA_TABLE_PATH + \"/\" + \"store_orders\")\ndeltaTable.history().show(5)",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Go back and execute cell title \"Use Delta framework to either create a delta table or merge data into an existing table\"",
			"metadata": {
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "dfread=spark.read.format(\"delta\").load(DELTA_TABLE_PATH + \"/\" + \"store_orders\")\ndfread.createOrReplaceTempView( \"store_orders\")\nprint(dfread.count())\ndeltaTable = DeltaTable.forPath(spark, DELTA_TABLE_PATH + \"/\" + \"store_orders\")\ndeltaTable.history().show(5)",
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "%%sql\nSELECT * FROM store_orders WHERE order_number = 2601;",
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# In cell titled \"# Send arguments to the job\" change sys.argv+=[\"--INCREMENTAL_DATA_FOLDER\", \"2023/01/12/18\"] to sys.argv+=[\"--INCREMENTAL_DATA_FOLDER\", \"2023/01/12/19\"]\n# Run cell titled \"Send arguments to the job\" and \"Read job arguments\"\n# Go back and execute cell title \"Use Delta framework to either create a delta table or merge data into an existing table\"",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "dfreadprev=spark.read.format(\"delta\").option(\"versionAsOf\", \"0\").load(DELTA_TABLE_PATH + \"/\" + \"store_orders\")\ndfread.createOrReplaceTempView( \"store_orders_1\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "%%sql\nSELECT * FROM store_orders_1 WHERE order_number = 2345;",
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "\ndeltaTable.generate(\"symlink_format_manifest\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"CREATE DATABASE IF NOT EXISTS delta\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "SALES_ORDERS_SQL = \"CREATE EXTERNAL TABLE IF NOT EXISTS store_orders (dms_mode string,     \\\n                                                                      order_number int,    \\\n                                                                      customer_id int,     \\\n                                                                      product_id int,      \\\n                                                                      order_date string,   \\\n                                                                      units int,           \\\n                                                                      sale_price float,    \\\n                                                                      currency string,     \\\n                                                                      order_mode string)   \\\n                                                                      ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'  \\\n                                                                      STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat' \\\n                                                                      OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'   \\\n                                                                      LOCATION '\" + DELTA_TABLE_PATH + \"/\" + \"store_orders\" + \"/\" + \"_symlink_format_manifest'\"\n#print(SALES_ORDERS_SQL)\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"USE delta\")\nspark.sql(\"DROP TABLE IF EXISTS store_orders\")\nspark.sql(SALES_ORDERS_SQL)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dfread=spark.read.format(\"delta\").load(DELTA_TABLE_PATH + \"/\" + \"store_orders\").show(10)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+------------+-----------+----------+----------+-----+----------+--------+----------+\n|dms_mode|order_number|customer_id|product_id|order_date|units|sale_price|currency|order_mode|\n+--------+------------+-----------+----------+----------+-----+----------+--------+----------+\n|       I|           1|        212|         5|02/03/2019|   10|      11.6|     USD|       NEW|\n|       I|           2|       1940|        10|06/24/2020|    8|     72.31|     USD|       NEW|\n|       I|           3|         60|         6|02/11/2019|    4|     24.82|     INR|       NEW|\n|       I|           4|       2776|         6|05/20/2018|    4|     20.91|     USD|       NEW|\n|       I|           5|        409|         9|07/05/2019|    5|     98.41|     INR|       NEW|\n|       I|           6|        978|         6|12/16/2020|    1|       6.9|     USD|       NEW|\n|       I|           7|       2904|         6|01/04/2021|    1|     71.56|     EUR|       NEW|\n|       I|           8|       1269|         3|08/11/2018|    6|     47.67|     USD|       NEW|\n|       I|           9|       2628|         5|01/16/2017|    1|     59.05|     EUR|       NEW|\n|       I|          10|       1672|         8|08/01/2020|    3|     43.42|     USD|       NEW|\n+--------+------------+-----------+----------+----------+-----+----------+--------+----------+\nonly showing top 10 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}