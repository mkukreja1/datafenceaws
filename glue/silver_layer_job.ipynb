{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "\n# Glue Studio Notebook\nYou are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n\n## Available Magics\n|          Magic              |   Type       |                                                                        Description                                                                        |\n|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n| %region                     |  String      |  Specify the AWS region in which to initialize a session.                                                                                                 |\n| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0).                               |\n| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n| %etl                        |  String      |  Changes the session type to Glue ETL.                                                                                                                    |\n| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n| %stop_session               |              |  Stops the current session.                                                                                                                               |\n| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X.                                                                           |\n| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer.                      |",
			"metadata": {
				"editable": false,
				"deletable": false,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "# Setup Spark and Glue configurations\n\n%glue_version 3.0\n%spark_conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n%spark_conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\n%number_of_workers 2\n\n%%configure\n{\n  \"--datalake-formats\": \"delta\"\n}",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Setup Python and Spark libraries\nimport sys\nfrom awsglue.transforms import *\nfrom pyspark.sql.functions import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql import functions as fn\nfrom urllib.request import urlopen\nfrom pyspark.sql.functions import udf\nimport hashlib\nimport urllib.request\nfrom io import StringIO\n\nfrom delta.tables import *\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, array, ArrayType, DateType, TimestampType, FloatType\nimport json\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\n#spark = glueContext.spark_session\nspark = SparkSession \\\n            .builder \\\n            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n            .getOrCreate()\n\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Send arguments to the job\n\nsys.argv+=[\"--S3_BUCKET\", \"aws-analytics-course\"]\nsys.argv+=[\"--BRONZE_LAYER_NAMESPACE\", \"bronze/dms/sales\"]\nsys.argv+=[\"--BRONZE_LAYER_ECOMMERCE_NAMESPACE\", \"bronze/kinesis\"]\nsys.argv+=[\"--SILVER_LAYER_NAMESPACE\", \"silver\"]\nsys.argv+=[\"--JOB_DATE\", \"2023/01/12/19\"]\nsys.argv+=[\"--TABLES\", \"{\\\"store_orders\\\": \\\"currency\\\", \\\"store_customers\\\": \\\"country\\\", \\\"products\\\": \\\"product_category\\\"}\"]\nsys.argv+=[\"--JOIN_COLUMNS_DELTA\", \"{\\\"store_orders\\\": \\\"order_number\\\", \\\"store_customers\\\": \\\"email\\\",  \\\"products\\\": \\\"product_id\\\"}\"]\nsys.argv+=[\"--JOIN_COLUMNS_INCREMENTAL\", \"{\\\"store_orders\\\": \\\"order_number\\\", \\\"store_customers\\\": \\\"email\\\", \\\"products\\\": \\\"product_category\\\"}\"]\nsys.argv+=[\"--CURRENCY_CONVERSION_URL\", \"https://api.exchangerate-api.com/v4/latest/usd\"]\nsys.argv+=[\"--ECOMMERCE_LOGS_BUCKET\", \"aws-analytics-incoming\"]\nsys.argv+=[\"--ECOMMERCE_STREAM_DATE\", \"2023/01/31/18\"]\nsys.argv+=[\"--GLUE_SILVER_DATABASE\", \"electroniz_curated\"]\n\nargs = getResolvedOptions(sys.argv,[\"S3_BUCKET\", \"BRONZE_LAYER_NAMESPACE\", \"BRONZE_LAYER_ECOMMERCE_NAMESPACE\", \"SILVER_LAYER_NAMESPACE\", \"JOB_DATE\", \"TABLES\", \"JOIN_COLUMNS_DELTA\", \"JOIN_COLUMNS_INCREMENTAL\", \"CURRENCY_CONVERSION_URL\", \"ECOMMERCE_LOGS_BUCKET\", \"ECOMMERCE_STREAM_DATE\", \"GLUE_SILVER_DATABASE\"])",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Get data from currency conversion API\n\ncurrency_conversion_response = urlopen(args['CURRENCY_CONVERSION_URL'])\ncurrency_conversion_json = json.loads(currency_conversion_response.read())",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Function to get currency conversion rates\n\ndef get_currency_conversion(currency_conversion_json, currency):\n    return currency_conversion_json['rates'][currency]",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Function to covert currency to USD\n\ndef curate_sales_price(currency, sales_price):\n  if (currency != 'USD'):\n    curated_value = float(sales_price)/float(get_currency_conversion(currency_conversion_json, currency))\n    return float(curated_value)\n  else:\n    return float(sales_price)\ncurate_sales_price_udf = udf(curate_sales_price, FloatType())",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Function to mask PII data\n\ndef mask_value(column):\n  mask_value = hashlib.sha256(column.encode()).hexdigest()\n  return mask_value\n\nmask_udf = udf(mask_value, StringType())",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Function to convert IP octect\n\ndef ip_to_country(ip):\n  ipsplit = ip.split(\".\")\n  ip_number=16777216*int(ipsplit[0]) + 65536*int(ipsplit[1]) + 256*int(ipsplit[2]) + int(ipsplit[3])  \n  return ip_number\n\nip_to_country_udf = udf(ip_to_country, StringType())",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Function to read data from S3 and create a Spark dataframe\n\ndef get_dataframe(BRONZE_TABLE_PATH):\n    try:\n        df_read_data_incremental = spark.read                             \\\n                                            .option(\"header\", \"true\")         \\\n                                            .option(\"inferSchema\", \"true\")    \\\n                                            .csv(BRONZE_TABLE_PATH)\n\n        df_read_data_incremental=curate_columns(df_read_data_incremental, 'S')\n        df_read_data_incremental = df_read_data_incremental.drop('Op')\n        df_read_data_incremental.printSchema()\n        df_read_data_incremental.show(10)\n        return df_read_data_incremental\n    except:\n        return 0\n    ",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Function to merge data in a Delta Lake table\n\ndef merge_to_delta(SILVER_TABLE_PATH, BRONZE_TABLE_PATH, JOB_DATE, JOIN_COLUMN_DELTA, JOIN_COLUMN_INCREMENTAL):\n    DELTA_TABLE_ALIAS=\"delta_table\"\n    INCREMENTAL_TABLE_ALIAS=\"data_incremental\"\n    JOIN_CONDITION=DELTA_TABLE_ALIAS + \".\" + JOIN_COLUMN_DELTA + \"=\" + INCREMENTAL_TABLE_ALIAS + \".\" + JOIN_COLUMN_INCREMENTAL\n    df_read_data_incremental = get_dataframe(BRONZE_TABLE_PATH + \"/\" + JOB_DATE + \"/\" + \"*.csv\")\n    if df_read_data_incremental != 0:\n        deltaTable = DeltaTable.forPath(spark, SILVER_TABLE_PATH)\n        if deltaTable:\n            print(\"Delta table exists\")\n            deltaTable.alias(DELTA_TABLE_ALIAS).merge(\n                    source=df_read_data_incremental.alias(INCREMENTAL_TABLE_ALIAS),\n                    condition=fn.expr(JOIN_CONDITION)) \\\n                    .whenMatchedUpdateAll()            \\\n                    .whenNotMatchedInsertAll()         \\\n                    .execute()",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Function to create a delta lake table\n\ndef merge_data_to_delta(BRONZE_TABLE_PATH, SILVER_TABLE_PATH, JOB_DATE, TABLE, PARTITION_COLUMN, JOIN_COLUMN_DELTA, JOIN_COLUMN_INCREMENTAL):\n    try:   \n        deltaTable = DeltaTable.forPath(spark, SILVER_TABLE_PATH)\n        if deltaTable:\n            print(\"Delta table exists\")\n            merge_to_delta(SILVER_TABLE_PATH, BRONZE_TABLE_PATH, JOB_DATE, JOIN_COLUMN_DELTA, JOIN_COLUMN_INCREMENTAL)\n    except:\n        print(\"Delta table does not exist\")\n        df_read_data_full = get_dataframe(BRONZE_TABLE_PATH + \"/\" + \"LOAD00000001.csv\")\n        df_read_data_full.write.format(\"delta\").save(SILVER_TABLE_PATH)\n        merge_to_delta(SILVER_TABLE_PATH, BRONZE_TABLE_PATH, JOB_DATE, JOIN_COLUMN_DELTA, JOIN_COLUMN_INCREMENTAL)\n        deltaTable = DeltaTable.forPath(spark, SILVER_TABLE_PATH)\n        deltaTable.generate(\"symlink_format_manifest\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Function to covert email to lowercase\n\ndef curate_email(email):\n  curated_value = email.lower()\n  return curated_value\n\ncurate_email_udf = udf(curate_email, StringType())",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Function to covert countries to abbreviated version\n\ndef curate_country(country):\n  if (country == 'USA' or country == 'United States'):\n    curated_value = 'USA'\n  elif (country == 'UK' or country == 'United Kingdom'):\n    curated_value = 'UK'\n  elif (country == 'CAN' or country == 'Canada'):\n    curated_value = 'CAN'\n  elif (country == 'IND' or country == 'India'):\n    curated_value = 'IND'\n  else:\n    curated_value = country\n  return curated_value\n\ncurate_country_udf = udf(curate_country, StringType())",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Function to get curate columns\n\ndef curate_columns(df, mode):\n    if (\"order_date\" in df.columns) and mode=='S':\n        df = df.withColumn(\"order_date\", to_date(df.order_date,  'MM/dd/yyyy'))\n    if (\"order_date\" in df.columns) and mode=='E':\n        df = df.withColumn(\"order_date\", to_date(df.order_date,  'dd/MM/yyyy'))\n    if \"updated_at\" in df.columns:\n        df = df.withColumn(\"updated_at\", to_timestamp(df.updated_at,  'yyyy-MM-dd HH:mm:ss')) \n    if \"sale_price\" in df.columns:\n        df = df.withColumn('sale_price_usd',curate_sales_price_udf('currency', 'sale_price'))\n    if \"email\" in df.columns:\n        df = df.withColumn('email_curated',curate_email_udf('email')).drop('email').withColumnRenamed('email_curated', 'email')\n    if \"country\" in df.columns:\n        df = df.withColumn('country_curated',curate_country_udf('country')).drop('country').withColumnRenamed('country_curated', 'country')\n    if \"phone\" in df.columns:\n        df = df.withColumn('phone_masked',mask_udf('phone')).drop('phone').withColumnRenamed('phone_masked', 'phone')\n    if \"city\" in df.columns:\n        df = df.withColumn('city_masked',mask_udf('city')).drop('city').withColumnRenamed('city_masked', 'city')\n    if \"postalcode\" in df.columns:\n        df = df.withColumn('postalcode_masked',mask_udf('postalcode')).drop('postalcode').withColumnRenamed('postalcode_masked', 'postalcode')        \n    if \"credit_card\" in df.columns:\n        df = df.withColumn(\"credit_card\",df.credit_card.cast(StringType()))\n        df = df.withColumn('credit_card_masked',mask_udf('credit_card')).drop('credit_card').withColumnRenamed('credit_card_masked', 'credit_card')\n    if \"address\" in df.columns:\n        df = df.withColumn('address_masked',mask_udf('address')).drop('address').withColumnRenamed('address_masked', 'address')\n    return df",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Function to flatten JSON files\n\ndef flatten_df(nested_df):\n    flat_cols = [c[0] for c in nested_df.dtypes if c[1][:6] != 'struct']\n    nested_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == 'struct']\n\n    flat_df = nested_df.select(flat_cols +\n                               [fn.col(nc+'.'+c).alias(nc+'_'+c)\n                                for nc in nested_cols\n                                for c in nested_df.select(nc+'.*').columns])\n    return flat_df",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Function to create glue tables\n\ndef create_glue_table(TABLE, BUCKET, SILVER_LAYER_NAMESPACE, GLUE_SILVER_DATABASE, COLUMNS):\n    TABLE_PATH=\"s3a://\" + args['S3_BUCKET'] + \"/\" + args['SILVER_LAYER_NAMESPACE']  + \"/\" +TABLE\n    table_sql=f\"\"\"\n             CREATE EXTERNAL TABLE IF NOT EXISTS \"\"\" + args['GLUE_SILVER_DATABASE'] + \"\"\".\"\"\" + TABLE + \"\"\" \n             ( \"\"\" + COLUMNS + \"\"\")\n              ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n              STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n              OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n              LOCATION '\"\"\" + TABLE_PATH +  \"\"\"/_symlink_format_manifest/'\n      \"\"\"\n    #print(table_sql)\n    spark.sql(table_sql)",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Function to read raw data fro bronze layer and merge data for SALES tables\n\nTABLE_DICT = json.loads(args['TABLES'])\nJOIN_COLUMNS_DELTA_DICT = json.loads(args['JOIN_COLUMNS_DELTA'])\nJOIN_COLUMNS_INCREMENTAL_DICT = json.loads(args['JOIN_COLUMNS_INCREMENTAL'])\n\nfor TABLE in TABLE_DICT:\n    BRONZE_TABLE_PATH=\"s3a://\" + args['S3_BUCKET'] + \"/\" + args['BRONZE_LAYER_NAMESPACE'] + \"/\" + TABLE\n    SILVER_TABLE_PATH=\"s3a://\" + args['S3_BUCKET'] + \"/\" + args['SILVER_LAYER_NAMESPACE'] + \"/\" + TABLE\n    merge_data_to_delta(BRONZE_TABLE_PATH, SILVER_TABLE_PATH, args['JOB_DATE'], TABLE, TABLE_DICT[TABLE],  JOIN_COLUMNS_DELTA_DICT[TABLE], JOIN_COLUMNS_INCREMENTAL_DICT[TABLE])",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Read ecommerce logs from bronze layer and map IP addresses to countries  \n\nIPLOCATION_SCHEMA =[\n    ('ip1', IntegerType()),\n    ('ip2', IntegerType()),\n    ('country_code', StringType()),\n    ('country_name', StringType())\n]\n\nipfields = [StructField(*field) for field in IPLOCATION_SCHEMA]\nschema_iplocation = StructType(ipfields)\n\nIPLOCATION_PATH=\"s3a://\" + args['ECOMMERCE_LOGS_BUCKET'] + \"/\" + \"iplocation/\"\ndf_iplocation = spark.read.csv(IPLOCATION_PATH, schema=schema_iplocation)\ndf_iplocation.registerTempTable('iplocation')\n\nLOGS_SCHEMA =[\n    ('time', StringType()),\n    ('remote_ip', StringType()),\n    ('country_name', StringType()),\n    ('ip_number', IntegerType()),\n    ('request', StringType()),\n    ('response', StringType()),\n    ('agent', StringType())\n]\n\nlogfields = [StructField(*field) for field in LOGS_SCHEMA]\nschema_logs = StructType(logfields)\n\nECOMMERCE_LOGS_PATH=\"s3a://\" + args['ECOMMERCE_LOGS_BUCKET'] + \"/\" + \"ecommerce_logs/\"\ndf_ecommerce_logs = spark.read.json(ECOMMERCE_LOGS_PATH, schema=schema_logs)\ndf_ecommerce_logs = df_ecommerce_logs.withColumn('ip_number',ip_to_country_udf('remote_ip'))\ndf_ecommerce_logs = df_ecommerce_logs.withColumn(\"ip_number_int\", df_ecommerce_logs['ip_number'].cast('int')).drop('ip_number').withColumnRenamed('ip_number_int', 'ip_number')\ndf_ecommerce_logs.registerTempTable('ecommerce')\n\ndf_ecommerce_country = spark.sql(\"SELECT e.time, e.remote_ip, i.country_name, e.ip_number, e.request, e.response, e.agent \" \\\n                                 \" FROM ecommerce e JOIN iplocation i WHERE ip1 <= ip_number AND ip2 >= ip_number\")\n\nECOMM_LOGS_SILVER_TABLE_PATH=\"s3a://\" + args['S3_BUCKET'] + \"/\" + args['SILVER_LAYER_NAMESPACE'] + \"/\" + \"ecommerce_country\"\n\ntry:   \n    deltaTable = DeltaTable.forPath(spark, ECOMM_LOGS_SILVER_TABLE_PATH)\n    df_ecommerce_country.printSchema()\n    if deltaTable:\n        deltaTable.alias(\"logs\").merge(\n             df_ecommerce_country.alias(\"logs_incr\"),\n             \"logs.remote_ip = logs_incr.remote_ip\") \\\n            .whenNotMatchedInsertAll() \\\n            .execute()\n        \nexcept:\n    print(\"Delta table does not exist\")\n    df_ecommerce_country.write.format(\"delta\").save(ECOMM_LOGS_SILVER_TABLE_PATH)\n    deltaTable = DeltaTable.forPath(spark, ECOMM_LOGS_SILVER_TABLE_PATH)\n    deltaTable.generate(\"symlink_format_manifest\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Function to read ecommerce transactions from bronze layer and merge data to delta table\n\nBRONZE_ECOMMERCE_TABLE_PATH=\"s3a://\" + args['S3_BUCKET'] + \"/\" + args['BRONZE_LAYER_ECOMMERCE_NAMESPACE'] + \"/\" + args['ECOMMERCE_STREAM_DATE']\nSILVER_ECOMMERCE_TABLE_PATH=\"s3a://\" + args['S3_BUCKET'] + \"/\" + args['SILVER_LAYER_NAMESPACE'] + \"/\" + \"ecommerce_orders\"\n\ntry:\n    df_ecommerce_transactions = spark.read.parquet(BRONZE_ECOMMERCE_TABLE_PATH)\n    df_ecommerce_transactions=flatten_df(df_ecommerce_transactions)\n    df_ecommerce_transactions=df_ecommerce_transactions.drop('id', 'eventtype', 'subject', 'eventtime', 'dataversion') \\\n                                                       .withColumnRenamed('data_customer_name', 'customer_name') \\\n                                                       .withColumnRenamed('data_address', 'address') \\\n                                                       .withColumnRenamed('data_city', 'city') \\\n                                                       .withColumnRenamed('data_postalcode', 'postalcode') \\\n                                                       .withColumnRenamed('data_country', 'country') \\\n                                                       .withColumnRenamed('data_phone', 'phone') \\\n                                                       .withColumnRenamed('data_email', 'email') \\\n                                                       .withColumnRenamed('data_product_name', 'product_name') \\\n                                                       .withColumnRenamed('data_order_date', 'order_date') \\\n                                                       .withColumnRenamed('data_currency', 'currency') \\\n                                                       .withColumnRenamed('data_order_mode', 'order_mode') \\\n                                                       .withColumnRenamed('data_sale_price', 'sale_price') \\\n                                                       .withColumnRenamed('data_order_number', 'order_number') \n\n    df_ecommerce_transactions=curate_columns(df_ecommerce_transactions, 'E')\n    df_ecommerce_transactions=df_ecommerce_transactions.withColumn(\"order_number\",col(\"order_number\").cast(IntegerType())).withColumn(\"sale_price\",col(\"sale_price\").cast(FloatType()))\n    df_ecommerce_transactions.show(3)\n    df_ecommerce_transactions.printSchema()\n\n    try:   \n        deltaTable = DeltaTable.forPath(spark, SILVER_ECOMMERCE_TABLE_PATH)\n        if deltaTable:\n            print(\"Delta table exists\")\n            DELTA_TABLE_ALIAS=\"ecomm_delta_table\"\n            INCREMENTAL_TABLE_ALIAS=\"ecomm_delta_table_incremental\"\n            JOIN_CONDITION=DELTA_TABLE_ALIAS + \".\" + \"email\" + \"=\" + INCREMENTAL_TABLE_ALIAS + \".\" + \"email\"\n            deltaTable.alias(DELTA_TABLE_ALIAS).merge(\n                        source=df_ecommerce_transactions.alias(INCREMENTAL_TABLE_ALIAS),\n                        condition=fn.expr(JOIN_CONDITION)) \\\n                        .whenMatchedUpdateAll()            \\\n                        .whenNotMatchedInsertAll()         \\\n                        .execute()\n    except:\n        df_ecommerce_transactions.write.format(\"delta\").save(SILVER_ECOMMERCE_TABLE_PATH)\n        deltaTable = DeltaTable.forPath(spark, SILVER_ECOMMERCE_TABLE_PATH)\n        deltaTable.generate(\"symlink_format_manifest\")\nexcept:\n    print(\"No data found for that period\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Create a curation schema in glue catalog\n\nspark.sql(\"CREATE DATABASE IF NOT EXISTS \" + args['GLUE_SILVER_DATABASE'])",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Create tables in glue catalog\n\ncreate_glue_table(\"store_customers\", args['S3_BUCKET'], args['SILVER_LAYER_NAMESPACE'], args['GLUE_SILVER_DATABASE'], \"customer_id integer, customer_name string, updated_at timestamp ,email string, country string, phone string, city string, postalcode string, credit_card string, address string\")\ncreate_glue_table(\"products\", args['S3_BUCKET'], args['SILVER_LAYER_NAMESPACE'], args['GLUE_SILVER_DATABASE'], \"product_code integer, product_name string, product_category string, updated_at timestamp\")\ncreate_glue_table(\"store_orders\", args['S3_BUCKET'], args['SILVER_LAYER_NAMESPACE'], args['GLUE_SILVER_DATABASE'], \"order_number integer, customer_id integer, product_id integer, order_date date, units integer, sale_price double, currency string, order_mode string, updated_at timestamp, sale_price_usd float\")\ncreate_glue_table(\"ecommerce_country\", args['S3_BUCKET'], args['SILVER_LAYER_NAMESPACE'], args['GLUE_SILVER_DATABASE'], \"time string, remote_ip string, country_name string, ip_number integer, request string, response string, agent string\")\ncreate_glue_table(\"ecommerce_orders\", args['S3_BUCKET'], args['SILVER_LAYER_NAMESPACE'], args['GLUE_SILVER_DATABASE'], \"customer_name string, product_name string, order_date date, currency string, order_mode string, sale_price float, order_number integer, sale_price_usd float, email string, country string, phone string, city string, postalcode string,address string\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}